# -*- coding: utf-8 -*-
"""step3-EDA-modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18fA2x90fxSKOucTWCx1NwmETih90OFw8

# Visualize data to inspire modeling
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pytz
import seaborn as sns
df = pd.read_csv('merged_data.csv')

"""## Visualize the distribution of data

## Drop repeated videos that have been watched multiple times
"""

unique_rows = df.drop_duplicates(subset=['VideoID'], keep='first')
df = unique_rows[['VideoTitle', 'CategoryName', 'SubscriberCount', 'ViewCount', 'LikeCount', 'DislikeCount', 'Duration', 'LikeViewRatio']]

"""## Data cleaning"""

df = df[(np.isfinite(df['LikeViewRatio'])) & (df['LikeCount']>0) & (df['Duration']>0)]
variables = ['SubscriberCount', 'ViewCount', 'LikeCount', 'DislikeCount', 'Duration', 'LikeViewRatio']
fig, axes = plt.subplots(2, 3, figsize=(10, 6))
axes = axes.flatten()
for i, var in enumerate(variables):
    axes[i].hist(df[var], bins=30, density=True)
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Density')
    axes[i].set_title(f'Distribution of {var}')
plt.tight_layout()
plt.show()

"""Check extreme values"""

# Define the percentile range for extreme values
lower_percentile = 0.1
upper_percentile = 99.9

# Iterate over the variables of interest
variables = ['SubscriberCount', 'ViewCount', 'LikeCount', 'DislikeCount', 'Duration', 'LikeViewRatio']
for column in variables:
    lower_bound = np.percentile(df[column], lower_percentile)
    upper_bound = np.percentile(df[column], upper_percentile)
    extreme_rows = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    if not extreme_rows.empty:
        print(f"Variable: {column}")
        print(extreme_rows[['VideoTitle', column]])
        print("--------------------")

"""Exclude extreme values"""

variables = ['SubscriberCount', 'ViewCount', 'LikeCount', 'DislikeCount', 'Duration', 'LikeViewRatio']
lower_threshold = 1
upper_threshold = 99

filtered_df = df.copy()  # Create a copy of the DataFrame to store the filtered results
for variable in variables:
    # Calculate the lower and upper percentile thresholds for the variable
    lower_quantile = df[variable].quantile(lower_threshold / 100)
    upper_quantile = df[variable].quantile(upper_threshold / 100)
    filtered_df = filtered_df[(filtered_df[variable] >= lower_quantile) & (filtered_df[variable] <= upper_quantile)]

print(df.shape)
print(filtered_df.shape)

"""## Log transform the data and visualize it again"""

# Apply log transformation
epsilon = 1e-10  # to handle zero values
filtered_df['SubscriberCount_log'] = np.log(filtered_df['SubscriberCount'] + epsilon)
filtered_df['ViewCount_log'] = np.log(filtered_df['ViewCount'] + epsilon)
filtered_df['LikeCount_log'] = np.log(filtered_df['LikeCount'] + epsilon)
filtered_df['Duration_log'] = np.log(filtered_df['Duration'] + epsilon)
filtered_df['LikeViewRatio_log'] = np.log(filtered_df['LikeViewRatio'] + epsilon)
filtered_df.to_csv('data-formodel.csv',index=False)

"""Visualize the transformed variables"""

variables = ['SubscriberCount_log', 'ViewCount_log', 'LikeCount_log', 'Duration_log', 'LikeViewRatio_log']
plt.figure(figsize=(12, 8))
for i, variable in enumerate(variables):
    plt.subplot(2, 3, i+1)
    sns.histplot(filtered_df[variable], kde=True)
    plt.xlabel(variable)
plt.tight_layout()
plt.show()

"""## Explore relationship between features

## Explore the relationship between variables
"""

subset_df = filtered_df[['SubscriberCount_log', 'ViewCount_log', 'LikeCount_log', 'Duration_log', 'LikeViewRatio_log','CategoryName']]
plot_kws = {'s': 5, 'alpha': 0.5}
sns.pairplot(subset_df, hue='CategoryName', plot_kws=plot_kws)
plt.savefig('fig9-scatterplot-bycategory.png')
plt.show()

"""### Feature engineering: GLM to find the predictors for LikeCounts"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import os
import pickle

"""Generate additional feature"""

filtered_df['TitleLength'] = filtered_df['VideoTitle'].apply(len)

filtered_df.info()

model_configs = [
    {'name': 'm1-log', 'columns': ['SubscriberCount_log', 'ViewCount_log', 'Duration_log', 'TitleLength'], 'Ycol': 'LikeCount_log'},
    {'name': 'm2-orig', 'columns': ['SubscriberCount', 'ViewCount', 'Duration', 'TitleLength'], 'Ycol': 'LikeCount'},
    {'name': 'm3-log-category', 'columns': ['SubscriberCount', 'ViewCount', 'Duration', 'TitleLength', 'CategoryName'], 'Ycol': 'LikeCount'},
    {'name': 'm4-orig-category', 'columns': ['CategoryName', 'SubscriberCount_log', 'ViewCount_log', 'Duration_log', 'TitleLength'], 'Ycol': 'LikeCount_log'}
]
model_perform = {}
for config in model_configs:
    X = filtered_df[config['columns']]
    y = filtered_df[config['Ycol']]
    
    if 'CategoryName' in config['columns']:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        training_categories = set(X_train['CategoryName'])
        missing_categories = set(X_test['CategoryName']) - training_categories

        for category in missing_categories:
            X_train[f'Category_{category}'] = 0
            X_test[f'Category_{category}'] = 0

        encoded_categories_train = pd.get_dummies(X_train['CategoryName'], prefix='Category')
        encoded_categories_test = pd.get_dummies(X_test['CategoryName'], prefix='Category')

        X_train_encoded = pd.concat([X_train.drop('CategoryName', axis=1), encoded_categories_train], axis=1)
        X_test_encoded = pd.concat([X_test.drop('CategoryName', axis=1), encoded_categories_test], axis=1)

        imputer = SimpleImputer(strategy='mean')
        X_train_imputed = imputer.fit_transform(X_train_encoded)
        X_test_imputed = imputer.transform(X_test_encoded)

        model = LinearRegression()
        model.fit(X_train_imputed, y_train)

        y_train_pred = model.predict(X_train_imputed)
        y_test_pred = model.predict(X_test_imputed)

        train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)
        train_r2 = r2_score(y_train, y_train_pred)

        test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)
        test_r2 = r2_score(y_test, y_test_pred)

        print(config['name'])
        print("Training RMSE:", train_rmse)
        print("Training R-squared:", train_r2)
        print("Testing RMSE:", test_rmse)
        print("Testing R-squared:", test_r2)
        print("Features", X_train_encoded.columns.to_list())
        print("Beta", model.coef_)
        print("------------------------------------")

    else:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = LinearRegression()
        model.fit(X_train, y_train)

        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)
        train_r2 = r2_score(y_train, y_train_pred)

        test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)
        test_r2 = r2_score(y_test, y_test_pred)

        print(config['name'])
        print("Training RMSE:", train_rmse)
        print("Training R-squared:", train_r2)
        print("Testing RMSE:", test_rmse)
        print("Testing R-squared:", test_r2)
        print(X.columns)
        print(model.coef_)
        print("------------------------------------")

    # Save the model
    model_filename = f'{config["name"]}.pkl'
    with open(model_filename, 'wb') as file:
        pickle.dump(model, file)
    model_perform[config['name']] = {"Training RMSE": train_rmse,
                                      "Training R-squared": train_r2,
                                      "Testing RMSE": test_rmse,
                                      "Testing R-squared": test_r2,
                                      "Features": X_train_encoded.columns.tolist(),
                                      "Beta": model.coef_
                                      }

"""## Analyze feature contribution"""

model_name = 'm4-orig-category'
features = model_perform[model_name]['Features']
beta = model_perform[model_name]['Beta']
df = pd.DataFrame({'Features': features, 'Beta': beta})

df_pos = df[df['Beta']>0].sort_values(by='Beta',ascending=False)
print(df_pos)
print("------------------------------------")
df_neg = df[df['Beta']<0].sort_values(by='Beta',ascending=True)
print(df_neg)